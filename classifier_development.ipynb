{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Bidirectional, SimpleRNN, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Generator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch normalization and optimizer clipping were added to avoid exploding gradients\n",
    "def generate_model_dataset1(num_layers, bidirectional, rnn_type, dropout_rate,\n",
    "                            vocab_size, input_dim, hidden_dim):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=hidden_dim, input_length=input_dim))\n",
    "    \n",
    "    if rnn_type == 'SimpleRNN':\n",
    "        layer = SimpleRNN(units=hidden_dim, activation='tanh')\n",
    "    elif rnn_type == 'LSTM':\n",
    "        layer = LSTM(units=hidden_dim, activation='tanh')\n",
    "    if bidirectional == True:\n",
    "        layer = Bidirectional(layer)\n",
    "    model.add(layer)\n",
    "    model.add(BatchNormalization())\n",
    "    if dropout_rate != 0:\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        \n",
    "    for _ in range(num_layers):\n",
    "        model.add(Dense(units=hidden_dim, activation='tanh'))\n",
    "    \n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    optimizer = Adam(amsgrad=True)\n",
    "    optimizer.clipnorm=1\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Performance Metric Computation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(x_test, y_test, model):\n",
    "    \n",
    "    y_pred = np.round(model.predict(x_test))\n",
    "    \n",
    "    # calculate true positives, false positives, true negatives, false negatives\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    true_neg = 0\n",
    "    false_neg = 0\n",
    "    \n",
    "    for counter in range(len(y_test)):\n",
    "        if y_test[counter] == 1:\n",
    "            if y_pred[counter] == 1:\n",
    "                true_pos += 1\n",
    "            else:\n",
    "                false_neg += 1\n",
    "        else:\n",
    "            if y_pred[counter] == 0:\n",
    "                true_neg += 1\n",
    "            else:\n",
    "                false_pos += 1\n",
    "    \n",
    "    accuracy = (true_pos + true_neg) / len(y_test)\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    try:\n",
    "        f1_score = (2 * precision * recall) / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        f1_score = 1\n",
    "    \n",
    "    print('Accuracy:', '%.5f' % accuracy)\n",
    "    print('Precision:', '%.5f' % precision)\n",
    "    print('Recall:', '%.5f' % recall)\n",
    "    print('F1 Score:', '%.5f' % f1_score)\n",
    "    \n",
    "    return accuracy, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Dataset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 1 consists of the first 100 non-repeated consecutive API calls made when running a Windows executable. This dataset was sourced from https://www.kaggle.com/ang3loliveira/malware-analysis-datasets-api-call-sequences, and more details can be found in the paper at https://www.techrxiv.org/articles/Behavioral_Malware_Detection_Using_Deep_Graph_Convolutional_Neural_Networks/10043099/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash</th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_4</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>...</th>\n",
       "      <th>t_91</th>\n",
       "      <th>t_92</th>\n",
       "      <th>t_93</th>\n",
       "      <th>t_94</th>\n",
       "      <th>t_95</th>\n",
       "      <th>t_96</th>\n",
       "      <th>t_97</th>\n",
       "      <th>t_98</th>\n",
       "      <th>t_99</th>\n",
       "      <th>malware</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>071e8c3f8922e186e57548cd4c703a5d</td>\n",
       "      <td>112</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>298</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>208</td>\n",
       "      <td>56</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>33f8e6d08a6aae939f25a8e0d63dd523</td>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>b68abd064e975e1c6d5f25e748663076</td>\n",
       "      <td>16</td>\n",
       "      <td>110</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>112</td>\n",
       "      <td>123</td>\n",
       "      <td>65</td>\n",
       "      <td>112</td>\n",
       "      <td>123</td>\n",
       "      <td>65</td>\n",
       "      <td>113</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>72049be7bd30ea61297ea624ae198067</td>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>228</td>\n",
       "      <td>302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>c9b3700a77facf29172f32df6bc77f48</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>209</td>\n",
       "      <td>260</td>\n",
       "      <td>40</td>\n",
       "      <td>209</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               hash  t_0  t_1  t_2  t_3  t_4  t_5  t_6  t_7  \\\n",
       "0  071e8c3f8922e186e57548cd4c703a5d  112  274  158  215  274  158  215  298   \n",
       "1  33f8e6d08a6aae939f25a8e0d63dd523   82  208  187  208  172  117  172  117   \n",
       "2  b68abd064e975e1c6d5f25e748663076   16  110  240  117  240  117  240  117   \n",
       "3  72049be7bd30ea61297ea624ae198067   82  208  187  208  172  117  172  117   \n",
       "4  c9b3700a77facf29172f32df6bc77f48   82  240  117  240  117  240  117  240   \n",
       "\n",
       "   t_8  ...  t_91  t_92  t_93  t_94  t_95  t_96  t_97  t_98  t_99  malware  \n",
       "0   76  ...    71   297   135   171   215    35   208    56    71        1  \n",
       "1  172  ...    81   240   117    71   297   135   171   215    35        1  \n",
       "2  240  ...    65   112   123    65   112   123    65   113   112        1  \n",
       "3  172  ...   208   302   208   302   187   208   302   228   302        1  \n",
       "4  117  ...   209   260    40   209   260   141   260   141   260        1  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_dataset_1 = '../Dataset1/dynamic_api_call_sequence_per_malware_100_0_306.csv'\n",
    "dataset1 = pd.read_csv(path_to_dataset_1)\n",
    "dataset1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .values to convert to numpy array and drop index\n",
    "X_dataset1 = dataset1.drop(columns=['hash', 'malware']).values\n",
    "Y_dataset1 = dataset1['malware'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Unique API Calls: 264\n"
     ]
    }
   ],
   "source": [
    "# convert numbers to between 1 and 100 for use with embedding\n",
    "vocab_size = 0\n",
    "vocab_conversion_dict = {}\n",
    "for example in X_dataset1:\n",
    "    for api_call in example:\n",
    "        if not(api_call in vocab_conversion_dict):\n",
    "            vocab_conversion_dict[api_call] = vocab_size\n",
    "            vocab_size += 1\n",
    "print('Total Number of Unique API Calls:', vocab_size)\n",
    "\n",
    "# convert array based on values in dictionary\n",
    "# https://stackoverflow.com/questions/16992713/translate-every-element-in-numpy-array-according-to-key\n",
    "X_dataset1 = np.vectorize(vocab_conversion_dict.get)(X_dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape:\n",
      " (35100, 100) \n",
      " (35100,)\n",
      "Test Data Shape:\n",
      " (8776, 100) \n",
      " (8776,)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "X_train_dataset1, X_test_dataset1, Y_train_dataset1, Y_test_dataset1 = train_test_split(X_dataset1, Y_dataset1,\n",
    "                                                                                        test_size=0.2)\n",
    "print('Train Data Shape:\\n',X_train_dataset1.shape, '\\n', Y_train_dataset1.shape)\n",
    "print('Test Data Shape:\\n',X_test_dataset1.shape, '\\n', Y_test_dataset1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28080 samples, validate on 7020 samples\n",
      "Epoch 1/10\n",
      "28080/28080 [==============================] - 25s 898us/step - loss: 0.9413 - accuracy: 0.7529 - val_loss: 1.4810 - val_accuracy: 0.9764\n",
      "Epoch 2/10\n",
      "28080/28080 [==============================] - 26s 943us/step - loss: 0.7247 - accuracy: 0.9139 - val_loss: 1.6941 - val_accuracy: 0.9759\n",
      "Epoch 3/10\n",
      "28080/28080 [==============================] - 26s 917us/step - loss: 0.6052 - accuracy: 0.9409 - val_loss: 1.4956 - val_accuracy: 0.9798\n",
      "Epoch 4/10\n",
      "28080/28080 [==============================] - 27s 964us/step - loss: 0.5573 - accuracy: 0.9514 - val_loss: 1.2444 - val_accuracy: 0.9734\n",
      "Epoch 5/10\n",
      "28080/28080 [==============================] - 28s 988us/step - loss: 0.5720 - accuracy: 0.9605 - val_loss: 0.9588 - val_accuracy: 0.9655\n",
      "Epoch 6/10\n",
      "28080/28080 [==============================] - 26s 921us/step - loss: 0.5009 - accuracy: 0.9618 - val_loss: 0.6662 - val_accuracy: 0.9527\n",
      "Epoch 7/10\n",
      "28080/28080 [==============================] - 25s 904us/step - loss: 0.5077 - accuracy: 0.9688 - val_loss: 0.7451 - val_accuracy: 0.9523\n",
      "Epoch 8/10\n",
      "28080/28080 [==============================] - 25s 891us/step - loss: 0.4997 - accuracy: 0.9718 - val_loss: 1.1258 - val_accuracy: 0.9764\n",
      "Epoch 9/10\n",
      "28080/28080 [==============================] - 25s 905us/step - loss: 0.4594 - accuracy: 0.9711 - val_loss: 0.6360 - val_accuracy: 0.9198\n",
      "Epoch 10/10\n",
      "28080/28080 [==============================] - 25s 901us/step - loss: 0.4551 - accuracy: 0.9749 - val_loss: 1.8861 - val_accuracy: 0.9845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f8917bc0710>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate model and fit data\n",
    "dataset1_baseline = generate_model_dataset1(num_layers=0, bidirectional=False, rnn_type='LSTM', dropout_rate=0.5,\n",
    "                                            vocab_size=vocab_size, input_dim=100, hidden_dim=70)\n",
    "dataset1_baseline.fit(X_train_dataset1, Y_train_dataset1, batch_size=128, epochs=10,\n",
    "                      verbose=1, validation_split=0.2,\n",
    "                      callbacks = [EarlyStopping(monitor='val_loss', patience=3)],\n",
    "                      class_weight={0: int(42797/1079), 1: 1})  # account for class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98302\n",
      "Precision: 0.98687\n",
      "Recall: 0.99578\n",
      "F1 Score: 0.99131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9830218778486782,\n",
       " 0.9868711513884048,\n",
       " 0.9957796014067996,\n",
       " 0.9913053626655773)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test performance of model on test data\n",
    "compute_metrics(X_test_dataset1, Y_test_dataset1, dataset1_baseline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
